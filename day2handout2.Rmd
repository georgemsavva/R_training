---
title: "day2handout2"
author: "George Savva"
date: "07/01/2021"
output:
  html_document:
    df_print: paged
    theme: cosmo
    toc: true
    toc_depth: 1
---



```{r echo=FALSE}
exerciseNumber=0;
exercise <- function(x="") {
  exerciseNumber <<- exerciseNumber+1; return(sprintf("## Exercise %d. %s", exerciseNumber, x))
  }
sectionNumber=0;
section <- function(x="") {
  sectionNumber <<- sectionNumber+1; return(sprintf("# **Section %d. %s**", sectionNumber, x))
  }

subsection <- function(x="") {
  return(sprintf("## %s", x))
  }


exampleNumber=0;
example <- function(x="") {
  exampleNumber <<- exampleNumber+1; return(sprintf("## Worked Example %d. %s", exampleNumber, x))
  }

```


`r section("Day 2 Introduction")`

On day 1 we saw the basics of how R works, and introduced you to some basic graphics, tests and descriptive statistics.

Today we will get more practical, dealing with datasets and estimating and reporting linear models.

Specific learning objectives are
1. Tidy data
2. Data wrangling
3. Estimating and diagnosing a linear model

Tomorrow we'll get to the fun stuff - graphics with ggplot!

`r section("Stroke rehabilitation study")`

Here we will use R to answer some research questions arising from a real study.

The data are from a randomised clinical trial of a new rehabilition intervention (compared to standard post-stroke care) aimed at improving the walking speed of stroke survivors.  Better walking speed is a good indicator of general stroke recovery.

We have recorded the age and sex of each participant, the treatment allocation, the hospital department from which they were recruited and time they talk to walk over six meters.

Our research questions are:

* What are the mean and standard deviation of walking speed for treated and untreated participants?
* Does the treatment improve walking speed compared to controls?
* By how much, and how certain are we of this?
* Does age affect walking speed?
* Does sex affect walking speed?
* Does sex affect the success of the treatment?
* Was there any difference in treatment effect by department?

## Loading and exploring the data

First we should inspect the data that we have.  It has been supplied to us as an Excel sheet.

`r exercise("Tidy data")`

1. How should the walking speed data look in 'tidy' format?
2. Can you create this in Excel?

`r section("Making the data 'tidy' in R")`

In this case we might find it easier to create our dataset in the right format before we import it into R.

But for more complex or larger datasets, or if we might have to repeat this operation, it might be better to do it in code.

We have our data in three separate sheets, and we need to end up with a single dataframe that allows us to answer our questions by expression relationships using the formula syntax.

That is, if we want to find how  be able to say something like:

`speed ~ treatment + age + sex + department`

so we need to set up our dataset to facilitate this.

## Step 1.  Appending datasets

Base R has the function `rbind()` to combine datafraes rowwise into a new longer dataframe.  But as with many 'data wrangling' problems, the tidyverse function `bind_rows()`is better.

Tidyverse is made of lots of different packages, we have already seen `readxl` in day 1, today we will use the `dplyr` package.

`r exercise("Installing a package")`


1. Check whether you have `dplyr` already installed, and install it if you do not.
2. Start a new script which will include all the command for creating the 'combined' dataset.
2. Load the `dplry` library, and look at the help for `bind_rows()`
3. Add the `readxl` commands to read the treatment and control data into two separate data frames.
4. (if you get time) Now use `bind_rows()` to turn them into a single dataframe.


`r example("Using bind_rows() ")`

If you didn't already do it as part of the exercise, load the control and treatment data into R from the Excel sheet.

```{r }
library(readxl) 
treated <- read_excel(path="day 2/walkingspeed.xlsx", sheet="treated")
control <- read_excel(path="day 2/walkingspeed.xlsx", sheet="control")
```

`r exercise("Type trouble")` 

1. What are the classes of each of the vectors in the `treated` and `control` data frames.  
2. Why has this happened?  
3. Is this a problem?  How should we fix it?

Now if we use `bind_rows()` on these datasets, we see the following:

```{r }
library(dplyr) 
combined <- bind_rows(treated, control)
head(combined)
```


We have two different columns for time!  This is because `bind_rows` tries to match the columns by name, and the `time` columns had different names in our dataset.  So we need to fix the names in our data frames to be the same before we bind them:

We can get or set the column names of a data frame with the `names()` function.

So here we will use:

```{r }
names(control) <- names(treated) # can you explain what this does?
```

Now what happens if you try to bind the rows?

```{r error=TRUE}
combined <- bind_rows(treated, control) # can you explain what this does?
```

If you didn't fix the type of the walking  earlier, `bind_rows()` will fail because in one dataset `time` is a character vector, and in the other it is a numeric vector.

Since we are going to be doing a statistical analysis on this variable, we need it to be a numeric.  Recall that we can convert a character to a numeric with `as.numeric()`

The same function will work for a vector that is part of a dataframe:

```{r }
treated$time <- as.numeric(treated$time)
```

Note the warning here, R is telling us that there was a non-numeric character, and that it has been converted to missing.  We should check the original data to see if this is acceptable to us.

Now we can bind the rows of the dataset together:

```{r }
combined <- bind_rows(treated, control) # can you explain what this does?
head(combined)
```

This is fine, but now we don't know which data point came from which group.  Fortunately, `bind_rows()` has another argument that adds a column to the new dataset indicating which row it came from.

`r exercise("Adding an group column")`

1. Check the help for `bind_rows()` to find out how to add the grouping column.
2. Try it!  Call the new column `group` or something like that.

```{r echo=FALSE}
# can you explain what this does?
combined <- bind_rows(treat=treated, control=control, .id = "group") 
```

`r section("Merging datasets")`

So now we have a combined dataset including all of our outcome data and a grouping variable.

To complete our analysis we need the patient meta-data to also be included in this data frame.  Since the data are linked by an ID variable, we will use the `merge()` function to add this information:

First, we'll read the meta-data into a new data frame.  Then we'll merge the two together using the `merge()` function.  `by.x` and `by.y` tell `merge()` which variables are the ID variables in the first and second datasets respectively.  I hope this is self-explanatory, check the documentation if it is not!

```{r }
meta <- read_excel(path="day 2/walkingspeed.xlsx", sheet="meta")
merged <- merge(combined, meta, by.x = "patid", by.y = "patient")
head(merged)
```

So now we have a single dataframe in tidy format with everything we need to conduct our analysis!

Note that each row contains everything we need to know about each unit of observation, and each column corresponds to a specific variable.

This was a fairly simple example of data wrangling, but is fairly typical of the process I need to go through when I get a new dataset before I can start working on it.  

Be prepared to spend some time figuring out how to do this with your data, and learning the relevent functions from the `dplry`, `tidyr` and other `tidyverse` libraries.   Also, when you are designing your data collection process be aware of how your data ultimately needs to be used, and design your spreadsheets or other data entry systems accordingly.

`r section("Exploratory analysis")`

Now we have our data in R, the first thing we should do is an exploratory analysis.  The aims here are to:

1. Check that the data all looks OK
2. Think about how our formal analysis might work (eg parametric or non-parametric analysis)

The main tools we have for exploratory analysis are graphs and descriptive statistics.  We should also consider the pattern of missing data to see if this tells us anything important.

For my exploratory plots I'll use base graphics.  Later when we are making our report graphics we'll use `ggplot`. 

For a first plot, seeing all the data points is important:

```{r }
plot(time~age, data=merged)
boxplot(time~sex, data=merged)
```

What do we learn from these plots?

Do the big outlier look reasonable (given what we know about the experiment?)

What are our options for dealing with it?

`r section("Outliers and transformations")`

A transformation is often a good option for dealing with outliers or non-normal distributions.

```{r }
plot(time~age, log="y",data=merged)
```

Now we can see an unreasonably high value, and an unreasonably low one!  We also see that the data is quite neatly normal 

```{r }
library(readxl)
library(ggplot2)

alldata <- read_excel("day 2/walkingspeed.xlsx", sheet="combined", range="A1:F139")
head(alldata)
summary(alldata)
ggplot(alldata, aes(x=age, y=time, shape=treated)) + geom_point() + facet_wrap(~sex)
```

We'll put the graph onto a log scale:

```{r }
ggplot(alldata, aes(x=age, y=time, shape=treated)) + 
  geom_point() + 
  facet_wrap(~sex) + 
  scale_y_continuous(trans="log10")
```

Now we might decide to remove those outlying points, because they are likely to be highly influential in our modelling and they are likely to be wrong.

The subset() function returns the subset of a dataframe that meets the criteria in its second argument.

```{r }
alldata2 <- subset(alldata, time>1 & time<100)
```

Now we can plot the dataset without outliers:

```{r }
ggplot(alldata2, aes(x=age, y=time, shape=treated)) + 
  geom_point() + 
  facet_wrap(~sex) + 
  scale_y_continuous(trans="log10") 
```

What can you see from the graph?


## Estimating our model

Our first question concerned descriptive statistics around walking time amongst men and women.  We saw in the last session that R does not have a good built in way to make nice descriptive tables.  In the last session we saw the 'table1' package but now we can use the new tbl_summary() function from the gtsummary package to get these.

```{r }
library(gtsummary)

tbl_summary(alldata)

```

This is really nice!.  Something to note: tbl_summary has detected that 'department' has only four values so has treated it as a categorical variable. This is fine but in general R functions will not do this (as we will see later) so be careful.

We want our data stratified by treatment group, so we can use:

```{r warning=TRUE}
tbl_summary(alldata, by=treated)
```

We won't spend a lot of time on this table, but lets change the statistics displayed, and some of the row names, and drop the rows we don't want to include.  For more customisations see the tbl_summary vignette.

Take some time to study the tbl_summary command below, the tbl_summary help files and vignettes to see how these are specified:


```{r }
tbl1 <- tbl_summary(alldata2[,c("treated","time","age", "sex")], 
                    by=treated,
                    label=list(time  ~ "Time (s)", age ~ "Age (yrs)", sex~"Sex"),
                    statistic=list(time~"{mean} ({sd})"))
add_overall(tbl1)
```


## Regression modelling

First we will test the treatment effect on walking speed. We will use a linear regression model for this.  Make sure you understand the commands below:

```{r }
model1 <- lm( data = alldata2 , time ~ treated)
summary(model1)
```

How do you interept this model output?  

We could also satisfy ourselves that the linear regression here is the same as an unpaired t-test with the equal variances assumption.  

```{r }
t.test(data=alldata2, time~ treated, var.equal=TRUE)
```

So if these outcomes are the same you might wonder why we prefer the linear model function?  We should prefer the linear regression because it offers us a lot more flexibility later on.

## Checking the validity of the model

We should always check that the assumption underlying a linear model are met.  The assumptions are:

* The residuals (differences between observations and 'predicted' values) are identically normally distributed
* The observations are independent of each other

There is no statistical test for these assumptions, we need to use graphical methods to judge visually whether the first is likely to be reasonable, and our knowledge of the experimental design to know whether the second is true.

When you 'plot' a linear model object the plot() function makes graphs to help you check the distribution of residuals from the model:

```{r }
par(mfrow=c(2,2))
plot(model1)
```

The second graph shows a normal qqplot of residuals from the model.  If the times were normally distributed aroud their predicted values this would follow the straight dotted line.  As it is we can see a significant deviation; there are a lot of residuals that are a lot bigger than the model thinks they should be.  

The first and third graphs are less useful for this regression (because there are only two possible 'predicted' values) but they still illustrate that although the residuals are not normally distributed they do at least seem to be similarly distriuted across groups.

## Transformations and linear models

In the last section we considered two different transformations of the data for the sake of plotting.  We looked at the log transformation and the inverse transformation.  We could try to model log(time) or 1/time as a function of treatment, to see if these meet the assumptions of the regression model better.

We could create a new variable with the transformed values, or we can add the transformation to our model.  First we'll look at the log-transformation:

```{r }
model2 <- lm( data=alldata2 , log(time) ~ treated)
summary(model2)
par(mfrow=c(2,2))
plot(model2)
```


```{r }
model3 <- lm( data=alldata2 , 1/time ~ treated)
summary(model3)
par(mfrow=c(2,2))
plot(model3)
```


The final model looks the best.  It seems that if we model the inverse of time (speed) instead of time itself then the distribution of the residuals is close to normal.

How should we interpret the final model?

## Presenting the results

An advantage of regression models is that we get an estimate and confidence interval for our effect as well as a p-value.  This is a major disadvantge of analysis or reporting just be placing p-values on plots; by restricting ourselves to this we never get to discuss how much of a difference the treatment makes, and our certainty around that estimate of effect.

The plain text summary of the model gives us all of the information we need, but there are other packages to organise regression model output in a more comprehensible and publication-ready form:

```{r }
library(sjPlot)
tab_model(model1, model2, model3)

tbl_regression(model1, intercept = TRUE, )
```

## A continuous predictor

We can add the effect of age into our model, by changing the model formula in the lm() call:

```{r }
model4 <- lm(data=alldata2, 1/time ~ treated + age)
tbl_regression(model4)
tab_model(model4)
```

It looks like the effect of age is 0!  But it is statistically significant, so the low effect size this is probably just a rounding error.  We'll have to change the level of precision being reported in the tabular output (and tweak another couple of options):

```{r }
tab_model(model4, digits = 4)
tbl_regression(model4,  
               show_single_row="treated",
               intercept=TRUE,
               estimate_fun = function(x) style_ratio(x, digits = 4))
```

We should continue to check that the model assumptions are still met.

```{r }
par(mfrow=c(2,2))
plot(model4)
```


## Plotting the model equations 

To plot the fitted values from our model we need to get convert the model object into a dataframe that can be plotted.

Before I do this I will make a new variable 'speed' corresponding to our transformed outcome.  We can use this instead of doing the transformation in the model equation.

predict() creates predictions from a model object, and can calculate confidence or prediction intervals.  I will use cbind to stick the predictions and the existing data together.

```{r }

alldata2$speed <- 6 / alldata2$time
head(alldata2)
model5 <- lm(data=alldata2, speed ~ age + treated + sex)
summary(model5)

model5predictions <- predict(model5, interval = "confidence", newdata = alldata2 ) # its helpful to specify 'newdata' in predict.
alldata3 <- cbind(alldata2,model5predictions)

head(alldata3)

ggplot(alldata3, aes(x=age, y=speed, shape=treated)) + 
  geom_point() + 
  facet_wrap(~sex) + 
  geom_ribbon(aes(ymin=lwr, ymax=upr, fill=treated), alpha=0.2) + 
  geom_line(aes(y=fit))
  

```


## Testing interactions

Our existing model does not allow the effect of treatment on walking speed to vary with sex.  But we might be interested in whether the effect is the same in men or women (a so called 'interaction' effect).  

Note it is not valid to do this by comparing models estimated in men and women separately.  We should instead estimate a model that includes the interaction between sex and treatment on walking speed.

```{r }
model6 <- lm(data=alldata2, speed ~ age + treated*sex)
summary(model6)
tab_model(model6)
```

Look at the plot of model6 compared to model5:

```{r }
# its helpful to specify 'newdata' in predict.
model6predictions <- predict(model6, interval = "confidence", newdata = alldata2 ) 
head(model6predictions)

alldata3 <- cbind(alldata2,model6predictions)

head(alldata3)

ggplot(alldata3, aes(x=age, y=speed, shape=treated)) + 
  geom_point() + 
  facet_wrap(~sex) + 
  geom_ribbon(aes(ymin=lwr, ymax=upr, fill=treated), alpha=0.2) + 
  geom_line(aes(y=fit))
```

Finally, we can test whether model6 fits the data better than model5.  Anova can be used to compare the fit of two models, and give a p-value for whether the more complex model provides a significantly better fit than the simpler one.

```{r }
anova(model5, model6)
```



## A model with a categorial predictor

We might be interested in whether walking speed varies by department.  We could add the department variable to our regression model as follows:

```{r }
model7 <- lm(data=alldata2, speed ~ age + sex + treated + department)
summary(model7)
```

But notice that R has not recognised that the 'department' variable should be treated as a categorical variable.  To make sure that 'department' is treated as categorical we should make a new variable in our data frame:



```{r }
alldata2$department_category <- factor(alldata2$department)
summary(alldata2)
```

Note that in the summary of our dataframe, 'department_factor' is now treated appropriately.  Lets see how the regression output changes:

```{r }
model7 <- lm(data=alldata2, speed ~ age + sex + treated + department_category)
summary(model7)
tab_model(model7)
```

R has given us an estimate of the effect of each department, compared to department 1, with a confidence interval and p-value.  It is probably a better question to ask whether the addition of 'department' led to a better model, that is, ask for an omnibus test of effect of 'department'.

```{r }
anova(model5, model7)
```

Finally, we might also be interested in comparisons between each pair of levels.  The best way to get this is via the 'emmeans' package as follows:

```{r }
library(emmeans)

pairs(emmeans(model7, ~department_category))

```

The output from emmeans includes a 'marginal' estimate for the walking speed in each department, plus an estimate and statistical test for each department compared to every other, with a suitable p-value correction for multiple testing.

## Exericse:

Can you use lm to test whether walking speed varies with department, and whether the effect of treatment on walking speed varies with department?



## Extensions to other models

Almost every experiment you do can be analysed with this paradigm, that is an outcome variable depending on one or more predictors.  And so data from almost every experiment can be analysed and reported with lm() or a related function.

In practice the modelling functions I find useful for most analyses are:

* lm() - regression models and ANOVA
* glm() - generalised linear models (count data and binary outcomes)
* lmer() and glmer() - from the lme4 package for mixed effects models (when the assumption of independence is not met, analogous to repeated measures ANOVA)
* nlme() for non-linear models

## Further reading on analysis with R

More detailed linear modelling tutorial.  http://tutorials.iq.harvard.edu/R/Rstatistics/Rstatistics.html

Understanding the linear regression diagnostic plots:  http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/

Using emmeans to get contrasts and margins https://aosmith.rbind.io/2019/03/25/getting-started-with-emmeans/





`r section("Subsetting")`



`r example("Transforming and combining variables")`

Just as we can apply mathematical functions to objects representing single numbers and vectors, we can apply them to columns in our data frame to transform or combine variables into new variables.

For example, suppose we want to log transform the height variable, and create a new variable called logHeight.  Then we would use:

```{r }
TreeData <- read_excel(path="day1/introstat.xlsx", sheet="P1-TreeSpeciesData", .name_repair = "universal")

TreeData$logHeight <- log(TreeData$height)
```

As when dealing with vectors above, R will apply the function individually to each element of the column, creatig a new variable in our data.

Check that this variable has been created.

Now we can plot the new variable against dgl:
```{r }
plot( logHeight ~ dgl, data = TreeData)
```


`r section("Linear regression")`

In the previous section we saw that dgl could be used to predict tree height.  We can estimate a linear model for this relationship using the lm() function

```{r }
lm1 <-  lm( height ~ dgl, data=TreeData  )
```

Note this function uses the same 'formula' interface as the t-test, boxplot functions above.

`lm()` creates a ‘linear model’ object.  The linear model object contains the all the details of an estimated linear regression model with ‘height’ as the outcome and ‘dgl’ as the predictor.

There are now lots of different functions you can use to extract elements of this model.
To see a description of all the information contained within the object, use:

```{r }
attributes(lm1)
```

But generally you won’t need to use these attributes directly.  Different functions exist to extract the information in readable and usable formats:

The summary command that we used earlier can also ‘summarise’ a linear model.  It produces the standard linear regression output that you would expect from any statistics package.

```{r }
summary(lm1)
```

To get the corresponding analysis of variance table, use the anova() function:

```{r }
anova(lm1) # Note by default R returns ‘type III’ ANOVA.
```

To plot the data with the regression model added, use the ‘plot’ function as previously, then abline() to add the line:

```{r }
plot( height ~ dgl, data = TreeData)
abline(lm1, col="red")
```


`r example("Regression predictions and diagnostics")`

After estimating a regression model we should inspect it to check its underlying assumptions are met.   R will show you some useful diagnostic plots if you run:

```{r }
plot(lm1)
```

If we want to directly check the predicted values or residuals from the model we can use the predict and resid functions

```{ r}
predict(lm1)
resid(lm1)
```

These functions generate vectors with each element corresponding to a row in the original data frame.  So element [1] of the vector created by predict() is the predicted (fitted) value of height for the first row in TreeData under our model.

`r example("Using factors to represent categorical variables")`

So far we have not considered how R represents categorial variables.  R has a special type called a 'factor' whereby categories are represented by integers corresponding to groups, with each integer given a text label.  We'll illustrate their use with a simple example:

## A model with a categorical predictor

Suppose we wanted to model the relationship between tree height and species.  'Species' is a categorical variable, rather than a continuous numeric as dgl was, but the way in which we estimate the model in R is the same. That is, we do this by running:

```{r }
lm2 <-  lm( height ~ species.name, data=TreeData  )
summary(lm2)
```

Notice that we did not need to create any 'dummy' variables to indicate the different levels, R did this automatically, and chose the level with the lowest value (alphabetically) to be the reference value. 

Because the species.name variable was a ‘character’ variable, R understood that we wanted it to be analysed in categories (just as it did when we estimated the box plots).

Much of the time, if we try to use a character variable in an analysis, R will interpret this as a factor.

If we had used the numeric species number variable, then R would not have automatically realised this, and we would get a non-sensical regression.

```{r }
lm3 <-  lm( height ~ species, data=TreeData  )
summary(lm3)
```

To make R treat a numeric variable as categorical it needs to be turned into a ‘factor’ variable.  A factor is a numeric variable where the numbers just represent groups, they lose their meaning as numbers.

Rather than overwriting the species numeric variable, we’ll create a new one for the factor:

First we’ll check the class of TreeData$species and ask for a summary:

```{r }
class(TreeData$species)
summary(TreeData$species)
```

We see a summary that is suitable for a continuous numeric (mean, median etc)

Now make a new factor variable and add it to the dataframe:

```{r }
TreeData$speciesF <- factor(TreeData$species)
```

Now check the class and the summary again.  R realises that a factor should be summarised by a tabulation not mean, median etc.

```{r }
class(TreeData$speciesF)
summary(TreeData$speciesF)
```

Finally the regression model should look sensible again, that is with one row per category (compared to a reference group).

```{r }
lm4 <- lm( height ~ speciesF, data=TreeData  )
summary(lm4)
```

Exercise
a)	Are the regression coefficients the same as those estimated for model 2?  If not, why not?
b)	The function `tbl_regression()` in the `gtsummary` package provides nicely laid out summaries of regression models.  Give it a try.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

